{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import spacy\r\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "from flair.data import Sentence\r\n",
    "from flair.embeddings import TransformerDocumentEmbeddings, WordEmbeddings, DocumentPoolEmbeddings\r\n",
    "from tqdm import tqdm\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore') #don't want warnings in my notebook output on github\r\n",
    "tqdm.pandas(desc=\"tqdm bar!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def spacy_tokenizer(text):\r\n",
    "    tokens = nlp(text)\r\n",
    "    # Lemmatizing each token, converting each token into lowercase and removing puncutation and named entities\r\n",
    "    tokens = [ word.lemma_.lower().strip() for word in tokens if not word.is_punct and not word.ent_type_ ]\r\n",
    "    # Removing stop words\r\n",
    "    tokens = [ word for word in tokens if word not in STOP_WORDS ]\r\n",
    "    return \" \".join(tokens)\r\n",
    "\r\n",
    "def one_hot(x, num_classes):\r\n",
    "    one_hot_vec = np.zeros(num_classes, dtype=float)\r\n",
    "    one_hot_vec[x] = 1\r\n",
    "    return one_hot_vec\r\n",
    "\r\n",
    "def embed_document(document, document_embeddings):\r\n",
    "    document = Sentence(document)\r\n",
    "    document_embeddings.embed(document)\r\n",
    "    return document.embedding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class JobDescDataset(Dataset):\r\n",
    "    def __init__(self, df, vec_col=\"text_vecs\"):\r\n",
    "        self.df = df\r\n",
    "        self.vec_col = vec_col\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.df)\r\n",
    "    \r\n",
    "    def __getitem__(self,idx):\r\n",
    "        X = self.df[self.vec_col].iloc[idx]\r\n",
    "        y = self.df.label_one_hot.iloc[idx]\r\n",
    "        return X, torch.from_numpy(y)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class Net(nn.Module):\r\n",
    "    def __init__(self, in_features:int, n_classes:int) -> None:\r\n",
    "        super(Net, self).__init__()\r\n",
    "\r\n",
    "        self.fc1 = nn.Linear(in_features, int(in_features))\r\n",
    "        self.fc2 = nn.Linear(in_features, int(in_features/4))\r\n",
    "        self.fc3 = nn.Linear(int(in_features/4), n_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def num_flat_features(self, x):\r\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\r\n",
    "        num_features = 1\r\n",
    "        for s in size:\r\n",
    "            num_features *= s\r\n",
    "        return num_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# load data\r\n",
    "csv_file = 'data\\document_type_data.csv'\r\n",
    "df = pd.read_csv(csv_file, index_col=[0])\r\n",
    "df.text = df.text.map(lambda x: \" \".join(eval(x)))\r\n",
    "\r\n",
    "# pre-processing\r\n",
    "nlp = spacy.load('en_core_web_sm')   \r\n",
    "print(\"spacy pre-processing\")\r\n",
    "df[\"text_pp\"] = df.text.progress_apply(spacy_tokenizer)\r\n",
    "\r\n",
    "to_predict = df[df.isnull().any(1)]\r\n",
    "df = df.dropna()\r\n",
    "# level to int and then one-hot vectors\r\n",
    "\r\n",
    "labels = df.label.unique()\r\n",
    "labels_dict = dict(zip(labels, range(len(labels))))\r\n",
    "df[\"class_name\"] = df.label\r\n",
    "df[\"label\"] = df.class_name.map(lambda x: labels_dict[x])\r\n",
    "df[\"label_one_hot\"] = df.label.map(lambda x: one_hot(x, num_classes=len(labels)))\r\n",
    "\r\n",
    "train, test = train_test_split(df.dropna(), random_state=42, stratify=df.label)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spacy pre-processing\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tqdm bar!: 100%|██████████| 100/100 [00:08<00:00, 11.73it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I will refrain from using the Longformer embeddings here since loading the model frequently exceeds the RAM capacity of my computer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# get text embeddings\r\n",
    "embedder = \"glove\"\r\n",
    "\r\n",
    "# init embedding\r\n",
    "if embedder == \"longformer\":\r\n",
    "    document_embeddings = TransformerDocumentEmbeddings('allenai/longformer-base-4096')\r\n",
    "elif embedder == \"glove\":\r\n",
    "    # glove pooling = mean\r\n",
    "    glove_embedding = WordEmbeddings('glove')\r\n",
    "    document_embeddings = DocumentPoolEmbeddings([glove_embedding])\r\n",
    "else:\r\n",
    "    raise ValueError(\"embedder has to be either longformer or glove\")\r\n",
    "\r\n",
    "train[\"text_vecs\"] = train[\"text\"].progress_apply(lambda x: embed_document(x, document_embeddings))\r\n",
    "test[\"text_vecs\"] = test[\"text\"].progress_apply(lambda x: embed_document(x, document_embeddings))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tqdm bar!: 100%|██████████| 75/75 [00:00<00:00, 103.10it/s]\n",
      "tqdm bar!: 100%|██████████| 25/25 [00:00<00:00, 93.81it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# set up training \r\n",
    "train_dataset = JobDescDataset(train)\r\n",
    "test_dataset = JobDescDataset(test)\r\n",
    "to_predict_dataset = JobDescDataset(to_predict)\r\n",
    "\r\n",
    "learning_rate = 0.01\r\n",
    "batch_size = 20\r\n",
    "epochs = 20\r\n",
    "\r\n",
    "training_generator = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\r\n",
    "test_generator = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\r\n",
    "predict_generator = DataLoader(to_predict_dataset, batch_size = batch_size, shuffle = False)\r\n",
    "\r\n",
    "net = Net(in_features=train.text_vecs[0].shape[0], n_classes=len(train.label.unique()))\r\n",
    "print(net)\r\n",
    "\r\n",
    "optimizer = optim.Adam(net.parameters(), lr = learning_rate)\r\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=25, bias=True)\n",
      "  (fc3): Linear(in_features=25, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# train loop\r\n",
    "net.train()\r\n",
    "for epoch in range(epochs):\r\n",
    "    running_loss = 0.0\r\n",
    "    for i, data in enumerate(training_generator):\r\n",
    "        inputs, labels = data\r\n",
    "        inputs, labels = (inputs).type(torch.FloatTensor), (labels)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        outputs = net(inputs)\r\n",
    "        loss = criterion(outputs, labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        running_loss += loss.item()\r\n",
    "    if (epoch+1) % 2 == 0 or epoch == 0:\r\n",
    "        total_predicted = []\r\n",
    "        total_labels = []\r\n",
    "\r\n",
    "        # get f1 score on test data\r\n",
    "        with torch.no_grad():\r\n",
    "            for data in test_generator:\r\n",
    "                inputs, labels = data\r\n",
    "                inputs = (inputs).type(torch.FloatTensor)\r\n",
    "                outputs = net(inputs)\r\n",
    "                _, predicted = torch.max(outputs.data, 1)\r\n",
    "                total_predicted.append(predicted.cpu().detach().numpy())\r\n",
    "\r\n",
    "                _, labels = torch.max(labels, 1)\r\n",
    "                total_labels.append(labels.cpu().detach().numpy())\r\n",
    "\r\n",
    "        total_predicted = np.hstack(total_predicted)\r\n",
    "        total_labels = np.hstack(total_labels)\r\n",
    "        f1 = f1_score(total_labels, total_predicted, average='micro')\r\n",
    "        \r\n",
    "        print(f'----------Epoch {epoch+1} Complete---------')\r\n",
    "        print(f\"loss: {running_loss/batch_size:.10f}\")\r\n",
    "        print(f\"f1: {f1:.2f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------Epoch 1 Complete---------\n",
      "loss: 0.2778021210\n",
      "f1: 0.60\n",
      "----------Epoch 2 Complete---------\n",
      "loss: 0.2525592533\n",
      "f1: 0.84\n",
      "----------Epoch 4 Complete---------\n",
      "loss: 0.1369852191\n",
      "f1: 0.72\n",
      "----------Epoch 6 Complete---------\n",
      "loss: 0.0779305924\n",
      "f1: 0.72\n",
      "----------Epoch 8 Complete---------\n",
      "loss: 0.0539636415\n",
      "f1: 0.88\n",
      "----------Epoch 10 Complete---------\n",
      "loss: 0.0387870896\n",
      "f1: 0.84\n",
      "----------Epoch 12 Complete---------\n",
      "loss: 0.0329400704\n",
      "f1: 0.84\n",
      "----------Epoch 14 Complete---------\n",
      "loss: 0.0263550883\n",
      "f1: 0.80\n",
      "----------Epoch 16 Complete---------\n",
      "loss: 0.0216115735\n",
      "f1: 0.88\n",
      "----------Epoch 18 Complete---------\n",
      "loss: 0.0172850475\n",
      "f1: 0.88\n",
      "----------Epoch 20 Complete---------\n",
      "loss: 0.0133499934\n",
      "f1: 0.92\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('workist-env': venv)"
  },
  "interpreter": {
   "hash": "afbace14a715144fd8b4a706fecd0cb77009e941a10c781b0dca6281a086e679"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}